{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们先看第一个读者的需求：原始数据有18个文件，取出每个文件中指定的几列，然后全部合并起来，<br>\n",
    "存储到一个新的文件，命名为2000_2017年省份碳排放数据。<br>\n",
    "\n",
    "\n",
    "结合文章讲解学习本代码更方便高效：[点击直达教程文章](https://mp.weixin.qq.com/s/878Co1CXMiSE7XdBwh7nxg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的数据包\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0、新建一个数据存储对象（我们用pandas中的Dataframe）<br>\n",
    "df_concat = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一个列表，存储时间\n",
    "date_year = [str(i) for i in range(2000, 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、读取目标Excel文件<br>\n",
    "file_path = 'data/2000年-2017年碳排放清单/2000年30个省份排放清单.xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、遍历取出每个sheet中需要的几行数据，存储到新建的Dataframe中<br>\n",
    "'''\n",
    "取出单个sheet中需要的数据\n",
    "'''\n",
    "def get_sheet_data(data, sheet_name, year):\n",
    "    # 取需要的几行数据\n",
    "    df_concat = data[sheet_name].loc[[2,3,48,49]]\n",
    "    # 给 Unnamed: 0 列进行重命名\n",
    "    df_concat = df_concat.rename(columns={'Unnamed: 0':'类别'})\n",
    "    # 插入两列数据 省份\t年份\n",
    "    df_concat.insert(loc=0,column='省份',value=sheet_name)\n",
    "    df_concat.insert(loc=1,column='年份',value=i)\n",
    "    # 将Total这列移动到第四列\n",
    "    df_temp = df_concat['Total']\n",
    "    df_concat = df_concat.drop(['Total'],axis=1)  # 先删除该列\n",
    "    df_concat.insert(loc=3,column='Total',value=df_temp)  # 然后插入到第四列位置\n",
    "    return df_concat\n",
    "\n",
    "'''\n",
    "取出单个Excel中需要的数据\n",
    "'''\n",
    "def get_excel_data(data, year):\n",
    "    df_concat = pd.DataFrame()\n",
    "    for sheet_name in list(data.keys()):\n",
    "        if sheet_name == 'Sum':\n",
    "            continue\n",
    "        df_temp = get_sheet_data(data, sheet_name, year)\n",
    "        df_concat = pd.concat([df_concat, df_temp])\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1620568255.1883297\n",
      "1620568272.432948\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "print(time.time())\n",
    "df_concat = pd.DataFrame()\n",
    "for i in date_year:\n",
    "    file_path = 'data/2000年-2017年碳排放清单/%s年30个省份排放清单.xlsx'%i\n",
    "    data = pd.read_excel(file_path, sheet_name=None)\n",
    "    df_temp = get_excel_data(data, i)\n",
    "    df_concat = pd.concat([df_concat, df_temp])\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2160, 25)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# 查看数据规模\n",
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "开始存储数据\n",
      "数据保存成功\n"
     ]
    }
   ],
   "source": [
    "print(\"开始存储数据\")\n",
    "df_concat.to_excel(\"data/2000_2017年省份碳排放数据.xlsx\", \"2000_2017\",index=None, encoding=\"utf-8\")\n",
    "# writer.save()\n",
    "print(\"数据保存成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "开始存储数据\n",
      "数据保存成功\n"
     ]
    }
   ],
   "source": [
    "# 完整代码\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "'''\n",
    "取出单个sheet中需要的数据\n",
    "'''\n",
    "def get_sheet_data(data, sheet_name, year):\n",
    "    # 取需要的几行数据\n",
    "    df_concat = data[sheet_name].loc[[2,3,48,49]]\n",
    "    # 给 Unnamed: 0 列进行重命名\n",
    "    df_concat = df_concat.rename(columns={'Unnamed: 0':'类别'})\n",
    "    # 插入两列数据 省份\t年份\n",
    "    df_concat.insert(loc=0,column='省份',value=sheet_name)\n",
    "    df_concat.insert(loc=1,column='年份',value=i)\n",
    "    # 将Total这列移动到第四列\n",
    "    df_temp = df_concat['Total']\n",
    "    df_concat = df_concat.drop(['Total'],axis=1)  # 先删除该列\n",
    "    df_concat.insert(loc=3,column='Total',value=df_temp)  # 然后插入到第四列位置\n",
    "    return df_concat\n",
    "\n",
    "'''\n",
    "取出单个Excel中需要的数据\n",
    "'''\n",
    "def get_excel_data(data, year):\n",
    "    df_concat = pd.DataFrame()\n",
    "    for sheet_name in list(data.keys()):\n",
    "        if sheet_name == 'Sum':\n",
    "            continue\n",
    "        df_temp = get_sheet_data(data, sheet_name, year)\n",
    "        df_concat = pd.concat([df_concat, df_temp])\n",
    "    return df_concat\n",
    "  \n",
    "# 0、新建一个数据存储对象（我们用pandas中的Dataframe）\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "# 生成一个列表，存储时间\n",
    "date_year = [str(i) for i in range(2000, 2018)]\n",
    "\n",
    "# 1、遍历取出每个Excel中的每个sheet中需要的几行数据，存储到新建的Dataframe中\n",
    "for i in date_year:\n",
    "    file_path = 'data/2000年-2017年碳排放清单/%s年30个省份排放清单.xlsx'%i\n",
    "    data = pd.read_excel(file_path, sheet_name=None)\n",
    "    df_temp = get_excel_data(data, i)\n",
    "    df_concat = pd.concat([df_concat, df_temp])\n",
    "\n",
    "# 2、写入数据\n",
    "print(\"开始存储数据\")\n",
    "df_concat.to_excel(\"data/2000_2017年省份碳排放数据.xlsx\", \"2000_2017\", index=None, encoding=\"utf-8\")\n",
    "print(\"数据保存成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看第二个读者的需求：原数据只有一个文件，里面有8个sheet，<br>\n",
    "需要将每个sheet中的几列取出来，然后根据日期存储为一个一个的csv文件。<br>\n",
    "\n",
    "结合文章讲解学习本代码更方便高效：[点击直达教程文章](https://mp.weixin.qq.com/s/878Co1CXMiSE7XdBwh7nxg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "meteo_china_tmin_2018\n53529\n53519\n53543\n53446\n53513\n53352\n53336\n53231\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          ymd        lat        lon       tmin\n",
       "0    20180101  51.024022  75.567044 -54.335985\n",
       "1    20180102   1.715915  79.802810 -41.935287\n",
       "2    20180103  64.537409  58.437196 -13.290637\n",
       "3    20180104   0.315354  43.847884 -93.777330\n",
       "4    20180105   6.556039  20.173486 -13.857637\n",
       "..        ...        ...        ...        ...\n",
       "360  20181227  94.531570  41.184321  -7.202198\n",
       "361  20181228  44.625031  87.786459 -82.287922\n",
       "362  20181229  26.811828  66.436228 -69.500716\n",
       "363  20181230  45.456553  17.346756 -31.927623\n",
       "364  20181231  46.135540  45.562136 -74.711277\n",
       "\n",
       "[2920 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ymd</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>tmin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20180101</td>\n      <td>51.024022</td>\n      <td>75.567044</td>\n      <td>-54.335985</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20180102</td>\n      <td>1.715915</td>\n      <td>79.802810</td>\n      <td>-41.935287</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20180103</td>\n      <td>64.537409</td>\n      <td>58.437196</td>\n      <td>-13.290637</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20180104</td>\n      <td>0.315354</td>\n      <td>43.847884</td>\n      <td>-93.777330</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20180105</td>\n      <td>6.556039</td>\n      <td>20.173486</td>\n      <td>-13.857637</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>20181227</td>\n      <td>94.531570</td>\n      <td>41.184321</td>\n      <td>-7.202198</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>20181228</td>\n      <td>44.625031</td>\n      <td>87.786459</td>\n      <td>-82.287922</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>20181229</td>\n      <td>26.811828</td>\n      <td>66.436228</td>\n      <td>-69.500716</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>20181230</td>\n      <td>45.456553</td>\n      <td>17.346756</td>\n      <td>-31.927623</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>20181231</td>\n      <td>46.135540</td>\n      <td>45.562136</td>\n      <td>-74.711277</td>\n    </tr>\n  </tbody>\n</table>\n<p>2920 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "1、读取、取出需要的数据并合并\n",
    "'''\n",
    "file_path = 'data/meteo_china_tmin_2018.xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=None)\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "2、遍历获取所有sheet中的数据\n",
    "'''\n",
    "for sheet_name in list(data.keys()):\n",
    "    print(sheet_name)\n",
    "    if sheet_name == 'meteo_china_tmin_2018':\n",
    "        continue\n",
    "    df_temp = data[sheet_name][['ymd', 'lat', 'lon', 'tmin']]\n",
    "    df_concat = pd.concat([df_concat, df_temp])\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20181001\n",
      "20181002\n",
      "20181003\n",
      "20181004\n",
      "20181005\n",
      "20181006\n",
      "20181007\n",
      "20181008\n",
      "20181009\n",
      "20181010\n",
      "20181011\n",
      "20181012\n",
      "20181013\n",
      "20181014\n",
      "20181015\n",
      "20181016\n",
      "20181017\n",
      "20181018\n",
      "20181019\n",
      "20181020\n",
      "20181021\n",
      "20181022\n",
      "20181023\n",
      "20181024\n",
      "20181025\n",
      "20181026\n",
      "20181027\n",
      "20181028\n",
      "20181029\n",
      "20181030\n",
      "20181031\n",
      "20181101\n",
      "20181102\n",
      "20181103\n",
      "20181104\n",
      "20181105\n",
      "20181106\n",
      "20181107\n",
      "20181108\n",
      "20181109\n",
      "20181110\n",
      "20181111\n",
      "20181112\n",
      "20181113\n",
      "20181114\n",
      "20181115\n",
      "20181116\n",
      "20181117\n",
      "20181118\n",
      "20181119\n",
      "20181120\n",
      "20181121\n",
      "20181122\n",
      "20181123\n",
      "20181124\n",
      "20181125\n",
      "20181126\n",
      "20181127\n",
      "20181128\n",
      "20181129\n",
      "20181130\n",
      "20181201\n",
      "20181202\n",
      "20181203\n",
      "20181204\n",
      "20181205\n",
      "20181206\n",
      "20181207\n",
      "20181208\n",
      "20181209\n",
      "20181210\n",
      "20181211\n",
      "20181212\n",
      "20181213\n",
      "20181214\n",
      "20181215\n",
      "20181216\n",
      "20181217\n",
      "20181218\n",
      "20181219\n",
      "20181220\n",
      "20181221\n",
      "20181222\n",
      "20181223\n",
      "20181224\n",
      "20181225\n",
      "20181226\n",
      "20181227\n",
      "20181228\n",
      "20181229\n",
      "20181230\n",
      "20181231\n",
      "20180101\n",
      "20180102\n",
      "20180103\n",
      "20180104\n",
      "20180105\n",
      "20180106\n",
      "20180107\n",
      "20180108\n",
      "20180109\n",
      "20180110\n",
      "20180111\n",
      "20180112\n",
      "20180113\n",
      "20180114\n",
      "20180115\n",
      "20180116\n",
      "20180117\n",
      "20180118\n",
      "20180119\n",
      "20180120\n",
      "20180121\n",
      "20180122\n",
      "20180123\n",
      "20180124\n",
      "20180125\n",
      "20180126\n",
      "20180127\n",
      "20180128\n",
      "20180129\n",
      "20180130\n",
      "20180131\n",
      "20180201\n",
      "20180202\n",
      "20180203\n",
      "20180204\n",
      "20180205\n",
      "20180206\n",
      "20180207\n",
      "20180208\n",
      "20180209\n",
      "20180210\n",
      "20180211\n",
      "20180212\n",
      "20180213\n",
      "20180214\n",
      "20180215\n",
      "20180216\n",
      "20180217\n",
      "20180218\n",
      "20180219\n",
      "20180220\n",
      "20180221\n",
      "20180222\n",
      "20180223\n",
      "20180224\n",
      "20180225\n",
      "20180226\n",
      "20180227\n",
      "20180228\n",
      "20180301\n",
      "20180302\n",
      "20180303\n",
      "20180304\n",
      "20180305\n",
      "20180306\n",
      "20180307\n",
      "20180308\n",
      "20180309\n",
      "20180310\n",
      "20180311\n",
      "20180312\n",
      "20180313\n",
      "20180314\n",
      "20180315\n",
      "20180316\n",
      "20180317\n",
      "20180318\n",
      "20180319\n",
      "20180320\n",
      "20180321\n",
      "20180322\n",
      "20180323\n",
      "20180324\n",
      "20180325\n",
      "20180326\n",
      "20180327\n",
      "20180328\n",
      "20180329\n",
      "20180330\n",
      "20180331\n",
      "20180401\n",
      "20180402\n",
      "20180403\n",
      "20180404\n",
      "20180405\n",
      "20180406\n",
      "20180407\n",
      "20180408\n",
      "20180409\n",
      "20180410\n",
      "20180411\n",
      "20180412\n",
      "20180413\n",
      "20180414\n",
      "20180415\n",
      "20180416\n",
      "20180417\n",
      "20180418\n",
      "20180419\n",
      "20180420\n",
      "20180421\n",
      "20180422\n",
      "20180423\n",
      "20180424\n",
      "20180425\n",
      "20180426\n",
      "20180427\n",
      "20180428\n",
      "20180429\n",
      "20180430\n",
      "20180501\n",
      "20180502\n",
      "20180503\n",
      "20180504\n",
      "20180505\n",
      "20180506\n",
      "20180507\n",
      "20180508\n",
      "20180509\n",
      "20180510\n",
      "20180511\n",
      "20180512\n",
      "20180513\n",
      "20180514\n",
      "20180515\n",
      "20180516\n",
      "20180517\n",
      "20180518\n",
      "20180519\n",
      "20180520\n",
      "20180521\n",
      "20180522\n",
      "20180523\n",
      "20180524\n",
      "20180525\n",
      "20180526\n",
      "20180527\n",
      "20180528\n",
      "20180529\n",
      "20180530\n",
      "20180531\n",
      "20180601\n",
      "20180602\n",
      "20180603\n",
      "20180604\n",
      "20180605\n",
      "20180606\n",
      "20180607\n",
      "20180608\n",
      "20180609\n",
      "20180610\n",
      "20180611\n",
      "20180612\n",
      "20180613\n",
      "20180614\n",
      "20180615\n",
      "20180616\n",
      "20180617\n",
      "20180618\n",
      "20180619\n",
      "20180620\n",
      "20180621\n",
      "20180622\n",
      "20180623\n",
      "20180624\n",
      "20180625\n",
      "20180626\n",
      "20180627\n",
      "20180628\n",
      "20180629\n",
      "20180630\n",
      "20180701\n",
      "20180702\n",
      "20180703\n",
      "20180704\n",
      "20180705\n",
      "20180706\n",
      "20180707\n",
      "20180708\n",
      "20180709\n",
      "20180710\n",
      "20180711\n",
      "20180712\n",
      "20180713\n",
      "20180714\n",
      "20180715\n",
      "20180716\n",
      "20180717\n",
      "20180718\n",
      "20180719\n",
      "20180720\n",
      "20180721\n",
      "20180722\n",
      "20180723\n",
      "20180724\n",
      "20180725\n",
      "20180726\n",
      "20180727\n",
      "20180728\n",
      "20180729\n",
      "20180730\n",
      "20180731\n",
      "20180801\n",
      "20180802\n",
      "20180803\n",
      "20180804\n",
      "20180805\n",
      "20180806\n",
      "20180807\n",
      "20180808\n",
      "20180809\n",
      "20180810\n",
      "20180811\n",
      "20180812\n",
      "20180813\n",
      "20180814\n",
      "20180815\n",
      "20180816\n",
      "20180817\n",
      "20180818\n",
      "20180819\n",
      "20180820\n",
      "20180821\n",
      "20180822\n",
      "20180823\n",
      "20180824\n",
      "20180825\n",
      "20180826\n",
      "20180827\n",
      "20180828\n",
      "20180829\n",
      "20180830\n",
      "20180831\n",
      "20180901\n",
      "20180902\n",
      "20180903\n",
      "20180904\n",
      "20180905\n",
      "20180906\n",
      "20180907\n",
      "20180908\n",
      "20180909\n",
      "20180910\n",
      "20180911\n",
      "20180912\n",
      "20180913\n",
      "20180914\n",
      "20180915\n",
      "20180916\n",
      "20180917\n",
      "20180918\n",
      "20180919\n",
      "20180920\n",
      "20180921\n",
      "20180922\n",
      "20180923\n",
      "20180924\n",
      "20180925\n",
      "20180926\n",
      "20180927\n",
      "20180928\n",
      "20180929\n",
      "20180930\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3、按时间进行分组，并保存为csv文件\n",
    "文件格式：hetao-ymd_tmin\n",
    "'''\n",
    "ymd_set = set(df_concat['ymd'])\n",
    "# 单文件测试\n",
    "# ymd_data = df_concat[df_concat['ymd']==20180716]\n",
    "# ymd_data.to_csv('./hetao-%d_tmin.csv'%20180716, index=False, header=None, columns=['lat', 'lon', 'tmin'])\n",
    "# 循环操作所以数据\n",
    "for ymd in ymd_set:\n",
    "    print(ymd)\n",
    "    ymd_data = df_concat[df_concat['ymd']==ymd]\n",
    "    ymd_data.to_csv('./data/hetao/hetao-%d_tmin.csv'%ymd, columns=['lat', 'lon', 'tmin'], header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#完整代码\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "读取、取出需要的数据并合并\n",
    "'''\n",
    "file_path = './data/meteo_china_tmin_2018.xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=None)\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for sheet_name in list(data.keys()):\n",
    "    if sheet_name == 'meteo_china_tmin_2018':\n",
    "        continue\n",
    "    df_temp = data[sheet_name][['ymd', 'lat', 'lon', 'tmin']]\n",
    "    df_concat = pd.concat([df_concat, df_temp])\n",
    "\n",
    "'''\n",
    "按时间进行分组，并保存为csv文件\n",
    "文件格式：hetao-ymd_tmin\n",
    "'''\n",
    "# 获取所有日期\n",
    "ymd_set = set(df_concat['ymd'])\n",
    "# 循环操作所有数据\n",
    "for ymd in ymd_set:\n",
    "    ymd_data = df_concat[df_concat['ymd']==ymd]\n",
    "    ymd_data.to_csv('./data/hetao/hetao-%d_tmin.csv'%ymd, columns=['lat', 'lon', 'tmin'], header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python389jvsc74a57bd03b1827d4b6462cc460901af0bc0d075c933010817877a813d51f78a107cbf6e5",
   "display_name": "Python 3.8.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}